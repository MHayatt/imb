{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--gpu-id'], dest='gpu_id', nargs=None, const=None, default='0', type=<class 'str'>, choices=None, help='id(s) for CUDA_VISIBLE_DEVICES', metavar=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import models.cifar as models\n",
    "\n",
    "from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig\n",
    "\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch CIFAR10/100 Training')\n",
    "# Datasets\n",
    "parser.add_argument('-d', '--dataset', default='cifar10', type=str)\n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "# Optimization options\n",
    "parser.add_argument('--epochs', default=300, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--train-batch', default=128, type=int, metavar='N',\n",
    "                    help='train batchsize')\n",
    "parser.add_argument('--test-batch', default=100, type=int, metavar='N',\n",
    "                    help='test batchsize')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n",
    "                    metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--drop', '--dropout', default=0, type=float,\n",
    "                    metavar='Dropout', help='Dropout ratio')\n",
    "parser.add_argument('--schedule', type=int, nargs='+', default=[150, 225],\n",
    "                        help='Decrease learning rate at these epochs.')\n",
    "parser.add_argument('--gamma', type=float, default=0.1, help='LR is multiplied by gamma on schedule.')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                    help='momentum')\n",
    "parser.add_argument('--weight-decay', '--wd', default=5e-4, type=float,\n",
    "                    metavar='W', help='weight decay (default: 1e-4)')\n",
    "# Checkpoints\n",
    "parser.add_argument('-c', '--checkpoint', default='checkpoint', type=str, metavar='PATH',\n",
    "                    help='path to save checkpoint (default: checkpoint)')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "# Architecture\n",
    "parser.add_argument('--arch', '-a', metavar='ARCH', default='resnet20',\n",
    "                    choices=model_names,\n",
    "                    help='model architecture: ' +\n",
    "                        ' | '.join(model_names) +\n",
    "                        ' (default: resnet18)')\n",
    "parser.add_argument('--depth', type=int, default=29, help='Model depth.')\n",
    "parser.add_argument('--cardinality', type=int, default=8, help='Model cardinality (group).')\n",
    "parser.add_argument('--widen-factor', type=int, default=4, help='Widen factor. 4 -> 64, 8 -> 128, ...')\n",
    "parser.add_argument('--growthRate', type=int, default=12, help='Growth rate for DenseNet.')\n",
    "parser.add_argument('--compressionRate', type=int, default=2, help='Compression Rate (theta) for DenseNet.')\n",
    "# Miscs\n",
    "parser.add_argument('--manualSeed', type=int, help='manual seed')\n",
    "parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n",
    "                    help='evaluate model on validation set')\n",
    "#Device options\n",
    "parser.add_argument('--gpu-id', default='0', type=str,\n",
    "                    help='id(s) for CUDA_VISIBLE_DEVICES')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args('-a densenet --depth 100 --growthRate 12 --train-batch 64 --epochs 300 --schedule 150 225 --wd 1e-4 --gamma 0.1 --checkpoint checkpoints/cifar10/densenet-bc-100-12'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state = {k: v for k, v in args._get_kwargs()}\n",
    "\n",
    "# Validate dataset\n",
    "assert args.dataset == 'cifar10' or args.dataset == 'cifar100', 'Dataset can only be cifar10 or cifar100.'\n",
    "\n",
    "# Use CUDA\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Random seed\n",
    "if args.manualSeed is None:\n",
    "    args.manualSeed = random.randint(1, 10000)\n",
    "random.seed(args.manualSeed)\n",
    "torch.manual_seed(args.manualSeed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(args.manualSeed)\n",
    "\n",
    "best_acc = 0  # best test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, model, criterion, optimizer, epoch, use_cuda):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    end = time.time()\n",
    "\n",
    "    bar = Bar('Processing', max=len(trainloader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda(async=True)\n",
    "        inputs, targets = torch.autograd.Variable(inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data[0], inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "        top5.update(prec5[0], inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(trainloader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    )\n",
    "        bar.next()\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "def test(testloader, model, criterion, epoch, use_cuda):\n",
    "    global best_acc\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    bar = Bar('Processing', max=len(testloader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = torch.autograd.Variable(inputs, volatile=True), torch.autograd.Variable(targets)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(outputs.data, targets.data, topk=(1, 5))\n",
    "        losses.update(loss.data[0], inputs.size(0))\n",
    "        top1.update(prec1[0], inputs.size(0))\n",
    "        top5.update(prec5[0], inputs.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=len(testloader),\n",
    "                    data=data_time.avg,\n",
    "                    bt=batch_time.avg,\n",
    "                    total=bar.elapsed_td,\n",
    "                    eta=bar.eta_td,\n",
    "                    loss=losses.avg,\n",
    "                    top1=top1.avg,\n",
    "                    top5=top5.avg,\n",
    "                    )\n",
    "        bar.next()\n",
    "    bar.finish()\n",
    "    return (losses.avg, top1.avg)\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    global state\n",
    "    if epoch in args.schedule:\n",
    "        state['lr'] *= args.gamma\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = state['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset cifar10\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "global best_acc\n",
    "start_epoch = args.start_epoch  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "if not os.path.isdir(args.checkpoint):\n",
    "    mkdir_p(args.checkpoint)\n",
    "\n",
    "\n",
    "\n",
    "# Data\n",
    "print('==> Preparing dataset %s' % args.dataset)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "import cifar_custom\n",
    "\n",
    "if args.dataset == 'cifar10':\n",
    "    dataloader = cifar_custom.CIFAR10\n",
    "    num_classes = 10\n",
    "else:\n",
    "    dataloader = cifar_custom.CIFAR100\n",
    "    num_classes = 100\n",
    "\n",
    "\n",
    "trainset = dataloader(root='./data', train=True, download=True, transform=transform_train,imbalance=True)\n",
    "trainloader = data.DataLoader(trainset, batch_size=args.train_batch, shuffle=True, num_workers=args.workers)\n",
    "\n",
    "testset = dataloader(root='./data', train=False, download=False, transform=transform_test)\n",
    "testloader = data.DataLoader(testset, batch_size=args.test_batch, shuffle=False, num_workers=args.workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.001 , 0.001 , 0.001 , 0.001 , 0.001 , 0.0002, 0.0002, 0.0002,\n",
       "       0.0002, 0.0002])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> creating model 'densenet'\n",
      "    Total params: 0.77M\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print(\"==> creating model '{}'\".format(args.arch))\n",
    "if args.arch.startswith('resnext'):\n",
    "    model = models.__dict__[args.arch](\n",
    "                cardinality=args.cardinality,\n",
    "                num_classes=num_classes,\n",
    "                depth=args.depth,\n",
    "                widen_factor=args.widen_factor,\n",
    "                dropRate=args.drop,\n",
    "            )\n",
    "elif args.arch.startswith('densenet'):\n",
    "    model = models.__dict__[args.arch](\n",
    "                num_classes=num_classes,\n",
    "                depth=args.depth,\n",
    "                growthRate=args.growthRate,\n",
    "                compressionRate=args.compressionRate,\n",
    "                dropRate=args.drop,\n",
    "            )\n",
    "elif args.arch.startswith('wrn'):\n",
    "    model = models.__dict__[args.arch](\n",
    "                num_classes=num_classes,\n",
    "                depth=args.depth,\n",
    "                widen_factor=args.widen_factor,\n",
    "                dropRate=args.drop,\n",
    "            )\n",
    "elif args.arch.endswith('resnet'):\n",
    "    model = models.__dict__[args.arch](\n",
    "                num_classes=num_classes,\n",
    "                depth=args.depth,\n",
    "            )\n",
    "else:\n",
    "    model = models.__dict__[args.arch](num_classes=num_classes)\n",
    "\n",
    "# model = torch.nn.DataParallel(model).cuda()\n",
    "cudnn.benchmark = True\n",
    "print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(*list(model.children())[:-1])\n",
    "class Head_Net(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, mid_features, num_classes=10):\n",
    "        super(Head_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features,mid_features)\n",
    "        self.fc2 = nn.Linear(mid_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x, self.fc2(x)\n",
    "    \n",
    "HeadNet=Head_Net(342,128).cuda()\n",
    "model=nn.Sequential(model,HeadNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 24, 32, 32]             648\n",
      "       BatchNorm2d-2           [-1, 24, 32, 32]              48\n",
      "              ReLU-3           [-1, 24, 32, 32]               0\n",
      "            Conv2d-4           [-1, 48, 32, 32]           1,152\n",
      "       BatchNorm2d-5           [-1, 48, 32, 32]              96\n",
      "              ReLU-6           [-1, 48, 32, 32]               0\n",
      "            Conv2d-7           [-1, 12, 32, 32]           5,184\n",
      "        Bottleneck-8           [-1, 36, 32, 32]               0\n",
      "       BatchNorm2d-9           [-1, 36, 32, 32]              72\n",
      "             ReLU-10           [-1, 36, 32, 32]               0\n",
      "           Conv2d-11           [-1, 48, 32, 32]           1,728\n",
      "      BatchNorm2d-12           [-1, 48, 32, 32]              96\n",
      "             ReLU-13           [-1, 48, 32, 32]               0\n",
      "           Conv2d-14           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-15           [-1, 48, 32, 32]               0\n",
      "      BatchNorm2d-16           [-1, 48, 32, 32]              96\n",
      "             ReLU-17           [-1, 48, 32, 32]               0\n",
      "           Conv2d-18           [-1, 48, 32, 32]           2,304\n",
      "      BatchNorm2d-19           [-1, 48, 32, 32]              96\n",
      "             ReLU-20           [-1, 48, 32, 32]               0\n",
      "           Conv2d-21           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-22           [-1, 60, 32, 32]               0\n",
      "      BatchNorm2d-23           [-1, 60, 32, 32]             120\n",
      "             ReLU-24           [-1, 60, 32, 32]               0\n",
      "           Conv2d-25           [-1, 48, 32, 32]           2,880\n",
      "      BatchNorm2d-26           [-1, 48, 32, 32]              96\n",
      "             ReLU-27           [-1, 48, 32, 32]               0\n",
      "           Conv2d-28           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-29           [-1, 72, 32, 32]               0\n",
      "      BatchNorm2d-30           [-1, 72, 32, 32]             144\n",
      "             ReLU-31           [-1, 72, 32, 32]               0\n",
      "           Conv2d-32           [-1, 48, 32, 32]           3,456\n",
      "      BatchNorm2d-33           [-1, 48, 32, 32]              96\n",
      "             ReLU-34           [-1, 48, 32, 32]               0\n",
      "           Conv2d-35           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-36           [-1, 84, 32, 32]               0\n",
      "      BatchNorm2d-37           [-1, 84, 32, 32]             168\n",
      "             ReLU-38           [-1, 84, 32, 32]               0\n",
      "           Conv2d-39           [-1, 48, 32, 32]           4,032\n",
      "      BatchNorm2d-40           [-1, 48, 32, 32]              96\n",
      "             ReLU-41           [-1, 48, 32, 32]               0\n",
      "           Conv2d-42           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-43           [-1, 96, 32, 32]               0\n",
      "      BatchNorm2d-44           [-1, 96, 32, 32]             192\n",
      "             ReLU-45           [-1, 96, 32, 32]               0\n",
      "           Conv2d-46           [-1, 48, 32, 32]           4,608\n",
      "      BatchNorm2d-47           [-1, 48, 32, 32]              96\n",
      "             ReLU-48           [-1, 48, 32, 32]               0\n",
      "           Conv2d-49           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-50          [-1, 108, 32, 32]               0\n",
      "      BatchNorm2d-51          [-1, 108, 32, 32]             216\n",
      "             ReLU-52          [-1, 108, 32, 32]               0\n",
      "           Conv2d-53           [-1, 48, 32, 32]           5,184\n",
      "      BatchNorm2d-54           [-1, 48, 32, 32]              96\n",
      "             ReLU-55           [-1, 48, 32, 32]               0\n",
      "           Conv2d-56           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-57          [-1, 120, 32, 32]               0\n",
      "      BatchNorm2d-58          [-1, 120, 32, 32]             240\n",
      "             ReLU-59          [-1, 120, 32, 32]               0\n",
      "           Conv2d-60           [-1, 48, 32, 32]           5,760\n",
      "      BatchNorm2d-61           [-1, 48, 32, 32]              96\n",
      "             ReLU-62           [-1, 48, 32, 32]               0\n",
      "           Conv2d-63           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-64          [-1, 132, 32, 32]               0\n",
      "      BatchNorm2d-65          [-1, 132, 32, 32]             264\n",
      "             ReLU-66          [-1, 132, 32, 32]               0\n",
      "           Conv2d-67           [-1, 48, 32, 32]           6,336\n",
      "      BatchNorm2d-68           [-1, 48, 32, 32]              96\n",
      "             ReLU-69           [-1, 48, 32, 32]               0\n",
      "           Conv2d-70           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-71          [-1, 144, 32, 32]               0\n",
      "      BatchNorm2d-72          [-1, 144, 32, 32]             288\n",
      "             ReLU-73          [-1, 144, 32, 32]               0\n",
      "           Conv2d-74           [-1, 48, 32, 32]           6,912\n",
      "      BatchNorm2d-75           [-1, 48, 32, 32]              96\n",
      "             ReLU-76           [-1, 48, 32, 32]               0\n",
      "           Conv2d-77           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-78          [-1, 156, 32, 32]               0\n",
      "      BatchNorm2d-79          [-1, 156, 32, 32]             312\n",
      "             ReLU-80          [-1, 156, 32, 32]               0\n",
      "           Conv2d-81           [-1, 48, 32, 32]           7,488\n",
      "      BatchNorm2d-82           [-1, 48, 32, 32]              96\n",
      "             ReLU-83           [-1, 48, 32, 32]               0\n",
      "           Conv2d-84           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-85          [-1, 168, 32, 32]               0\n",
      "      BatchNorm2d-86          [-1, 168, 32, 32]             336\n",
      "             ReLU-87          [-1, 168, 32, 32]               0\n",
      "           Conv2d-88           [-1, 48, 32, 32]           8,064\n",
      "      BatchNorm2d-89           [-1, 48, 32, 32]              96\n",
      "             ReLU-90           [-1, 48, 32, 32]               0\n",
      "           Conv2d-91           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-92          [-1, 180, 32, 32]               0\n",
      "      BatchNorm2d-93          [-1, 180, 32, 32]             360\n",
      "             ReLU-94          [-1, 180, 32, 32]               0\n",
      "           Conv2d-95           [-1, 48, 32, 32]           8,640\n",
      "      BatchNorm2d-96           [-1, 48, 32, 32]              96\n",
      "             ReLU-97           [-1, 48, 32, 32]               0\n",
      "           Conv2d-98           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-99          [-1, 192, 32, 32]               0\n",
      "     BatchNorm2d-100          [-1, 192, 32, 32]             384\n",
      "            ReLU-101          [-1, 192, 32, 32]               0\n",
      "          Conv2d-102           [-1, 48, 32, 32]           9,216\n",
      "     BatchNorm2d-103           [-1, 48, 32, 32]              96\n",
      "            ReLU-104           [-1, 48, 32, 32]               0\n",
      "          Conv2d-105           [-1, 12, 32, 32]           5,184\n",
      "      Bottleneck-106          [-1, 204, 32, 32]               0\n",
      "     BatchNorm2d-107          [-1, 204, 32, 32]             408\n",
      "            ReLU-108          [-1, 204, 32, 32]               0\n",
      "          Conv2d-109           [-1, 48, 32, 32]           9,792\n",
      "     BatchNorm2d-110           [-1, 48, 32, 32]              96\n",
      "            ReLU-111           [-1, 48, 32, 32]               0\n",
      "          Conv2d-112           [-1, 12, 32, 32]           5,184\n",
      "      Bottleneck-113          [-1, 216, 32, 32]               0\n",
      "     BatchNorm2d-114          [-1, 216, 32, 32]             432\n",
      "            ReLU-115          [-1, 216, 32, 32]               0\n",
      "          Conv2d-116          [-1, 108, 32, 32]          23,328\n",
      "      Transition-117          [-1, 108, 16, 16]               0\n",
      "     BatchNorm2d-118          [-1, 108, 16, 16]             216\n",
      "            ReLU-119          [-1, 108, 16, 16]               0\n",
      "          Conv2d-120           [-1, 48, 16, 16]           5,184\n",
      "     BatchNorm2d-121           [-1, 48, 16, 16]              96\n",
      "            ReLU-122           [-1, 48, 16, 16]               0\n",
      "          Conv2d-123           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-124          [-1, 120, 16, 16]               0\n",
      "     BatchNorm2d-125          [-1, 120, 16, 16]             240\n",
      "            ReLU-126          [-1, 120, 16, 16]               0\n",
      "          Conv2d-127           [-1, 48, 16, 16]           5,760\n",
      "     BatchNorm2d-128           [-1, 48, 16, 16]              96\n",
      "            ReLU-129           [-1, 48, 16, 16]               0\n",
      "          Conv2d-130           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-131          [-1, 132, 16, 16]               0\n",
      "     BatchNorm2d-132          [-1, 132, 16, 16]             264\n",
      "            ReLU-133          [-1, 132, 16, 16]               0\n",
      "          Conv2d-134           [-1, 48, 16, 16]           6,336\n",
      "     BatchNorm2d-135           [-1, 48, 16, 16]              96\n",
      "            ReLU-136           [-1, 48, 16, 16]               0\n",
      "          Conv2d-137           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-138          [-1, 144, 16, 16]               0\n",
      "     BatchNorm2d-139          [-1, 144, 16, 16]             288\n",
      "            ReLU-140          [-1, 144, 16, 16]               0\n",
      "          Conv2d-141           [-1, 48, 16, 16]           6,912\n",
      "     BatchNorm2d-142           [-1, 48, 16, 16]              96\n",
      "            ReLU-143           [-1, 48, 16, 16]               0\n",
      "          Conv2d-144           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-145          [-1, 156, 16, 16]               0\n",
      "     BatchNorm2d-146          [-1, 156, 16, 16]             312\n",
      "            ReLU-147          [-1, 156, 16, 16]               0\n",
      "          Conv2d-148           [-1, 48, 16, 16]           7,488\n",
      "     BatchNorm2d-149           [-1, 48, 16, 16]              96\n",
      "            ReLU-150           [-1, 48, 16, 16]               0\n",
      "          Conv2d-151           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-152          [-1, 168, 16, 16]               0\n",
      "     BatchNorm2d-153          [-1, 168, 16, 16]             336\n",
      "            ReLU-154          [-1, 168, 16, 16]               0\n",
      "          Conv2d-155           [-1, 48, 16, 16]           8,064\n",
      "     BatchNorm2d-156           [-1, 48, 16, 16]              96\n",
      "            ReLU-157           [-1, 48, 16, 16]               0\n",
      "          Conv2d-158           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-159          [-1, 180, 16, 16]               0\n",
      "     BatchNorm2d-160          [-1, 180, 16, 16]             360\n",
      "            ReLU-161          [-1, 180, 16, 16]               0\n",
      "          Conv2d-162           [-1, 48, 16, 16]           8,640\n",
      "     BatchNorm2d-163           [-1, 48, 16, 16]              96\n",
      "            ReLU-164           [-1, 48, 16, 16]               0\n",
      "          Conv2d-165           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-166          [-1, 192, 16, 16]               0\n",
      "     BatchNorm2d-167          [-1, 192, 16, 16]             384\n",
      "            ReLU-168          [-1, 192, 16, 16]               0\n",
      "          Conv2d-169           [-1, 48, 16, 16]           9,216\n",
      "     BatchNorm2d-170           [-1, 48, 16, 16]              96\n",
      "            ReLU-171           [-1, 48, 16, 16]               0\n",
      "          Conv2d-172           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-173          [-1, 204, 16, 16]               0\n",
      "     BatchNorm2d-174          [-1, 204, 16, 16]             408\n",
      "            ReLU-175          [-1, 204, 16, 16]               0\n",
      "          Conv2d-176           [-1, 48, 16, 16]           9,792\n",
      "     BatchNorm2d-177           [-1, 48, 16, 16]              96\n",
      "            ReLU-178           [-1, 48, 16, 16]               0\n",
      "          Conv2d-179           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-180          [-1, 216, 16, 16]               0\n",
      "     BatchNorm2d-181          [-1, 216, 16, 16]             432\n",
      "            ReLU-182          [-1, 216, 16, 16]               0\n",
      "          Conv2d-183           [-1, 48, 16, 16]          10,368\n",
      "     BatchNorm2d-184           [-1, 48, 16, 16]              96\n",
      "            ReLU-185           [-1, 48, 16, 16]               0\n",
      "          Conv2d-186           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-187          [-1, 228, 16, 16]               0\n",
      "     BatchNorm2d-188          [-1, 228, 16, 16]             456\n",
      "            ReLU-189          [-1, 228, 16, 16]               0\n",
      "          Conv2d-190           [-1, 48, 16, 16]          10,944\n",
      "     BatchNorm2d-191           [-1, 48, 16, 16]              96\n",
      "            ReLU-192           [-1, 48, 16, 16]               0\n",
      "          Conv2d-193           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-194          [-1, 240, 16, 16]               0\n",
      "     BatchNorm2d-195          [-1, 240, 16, 16]             480\n",
      "            ReLU-196          [-1, 240, 16, 16]               0\n",
      "          Conv2d-197           [-1, 48, 16, 16]          11,520\n",
      "     BatchNorm2d-198           [-1, 48, 16, 16]              96\n",
      "            ReLU-199           [-1, 48, 16, 16]               0\n",
      "          Conv2d-200           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-201          [-1, 252, 16, 16]               0\n",
      "     BatchNorm2d-202          [-1, 252, 16, 16]             504\n",
      "            ReLU-203          [-1, 252, 16, 16]               0\n",
      "          Conv2d-204           [-1, 48, 16, 16]          12,096\n",
      "     BatchNorm2d-205           [-1, 48, 16, 16]              96\n",
      "            ReLU-206           [-1, 48, 16, 16]               0\n",
      "          Conv2d-207           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-208          [-1, 264, 16, 16]               0\n",
      "     BatchNorm2d-209          [-1, 264, 16, 16]             528\n",
      "            ReLU-210          [-1, 264, 16, 16]               0\n",
      "          Conv2d-211           [-1, 48, 16, 16]          12,672\n",
      "     BatchNorm2d-212           [-1, 48, 16, 16]              96\n",
      "            ReLU-213           [-1, 48, 16, 16]               0\n",
      "          Conv2d-214           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-215          [-1, 276, 16, 16]               0\n",
      "     BatchNorm2d-216          [-1, 276, 16, 16]             552\n",
      "            ReLU-217          [-1, 276, 16, 16]               0\n",
      "          Conv2d-218           [-1, 48, 16, 16]          13,248\n",
      "     BatchNorm2d-219           [-1, 48, 16, 16]              96\n",
      "            ReLU-220           [-1, 48, 16, 16]               0\n",
      "          Conv2d-221           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-222          [-1, 288, 16, 16]               0\n",
      "     BatchNorm2d-223          [-1, 288, 16, 16]             576\n",
      "            ReLU-224          [-1, 288, 16, 16]               0\n",
      "          Conv2d-225           [-1, 48, 16, 16]          13,824\n",
      "     BatchNorm2d-226           [-1, 48, 16, 16]              96\n",
      "            ReLU-227           [-1, 48, 16, 16]               0\n",
      "          Conv2d-228           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-229          [-1, 300, 16, 16]               0\n",
      "     BatchNorm2d-230          [-1, 300, 16, 16]             600\n",
      "            ReLU-231          [-1, 300, 16, 16]               0\n",
      "          Conv2d-232          [-1, 150, 16, 16]          45,000\n",
      "      Transition-233            [-1, 150, 8, 8]               0\n",
      "     BatchNorm2d-234            [-1, 150, 8, 8]             300\n",
      "            ReLU-235            [-1, 150, 8, 8]               0\n",
      "          Conv2d-236             [-1, 48, 8, 8]           7,200\n",
      "     BatchNorm2d-237             [-1, 48, 8, 8]              96\n",
      "            ReLU-238             [-1, 48, 8, 8]               0\n",
      "          Conv2d-239             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-240            [-1, 162, 8, 8]               0\n",
      "     BatchNorm2d-241            [-1, 162, 8, 8]             324\n",
      "            ReLU-242            [-1, 162, 8, 8]               0\n",
      "          Conv2d-243             [-1, 48, 8, 8]           7,776\n",
      "     BatchNorm2d-244             [-1, 48, 8, 8]              96\n",
      "            ReLU-245             [-1, 48, 8, 8]               0\n",
      "          Conv2d-246             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-247            [-1, 174, 8, 8]               0\n",
      "     BatchNorm2d-248            [-1, 174, 8, 8]             348\n",
      "            ReLU-249            [-1, 174, 8, 8]               0\n",
      "          Conv2d-250             [-1, 48, 8, 8]           8,352\n",
      "     BatchNorm2d-251             [-1, 48, 8, 8]              96\n",
      "            ReLU-252             [-1, 48, 8, 8]               0\n",
      "          Conv2d-253             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-254            [-1, 186, 8, 8]               0\n",
      "     BatchNorm2d-255            [-1, 186, 8, 8]             372\n",
      "            ReLU-256            [-1, 186, 8, 8]               0\n",
      "          Conv2d-257             [-1, 48, 8, 8]           8,928\n",
      "     BatchNorm2d-258             [-1, 48, 8, 8]              96\n",
      "            ReLU-259             [-1, 48, 8, 8]               0\n",
      "          Conv2d-260             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-261            [-1, 198, 8, 8]               0\n",
      "     BatchNorm2d-262            [-1, 198, 8, 8]             396\n",
      "            ReLU-263            [-1, 198, 8, 8]               0\n",
      "          Conv2d-264             [-1, 48, 8, 8]           9,504\n",
      "     BatchNorm2d-265             [-1, 48, 8, 8]              96\n",
      "            ReLU-266             [-1, 48, 8, 8]               0\n",
      "          Conv2d-267             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-268            [-1, 210, 8, 8]               0\n",
      "     BatchNorm2d-269            [-1, 210, 8, 8]             420\n",
      "            ReLU-270            [-1, 210, 8, 8]               0\n",
      "          Conv2d-271             [-1, 48, 8, 8]          10,080\n",
      "     BatchNorm2d-272             [-1, 48, 8, 8]              96\n",
      "            ReLU-273             [-1, 48, 8, 8]               0\n",
      "          Conv2d-274             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-275            [-1, 222, 8, 8]               0\n",
      "     BatchNorm2d-276            [-1, 222, 8, 8]             444\n",
      "            ReLU-277            [-1, 222, 8, 8]               0\n",
      "          Conv2d-278             [-1, 48, 8, 8]          10,656\n",
      "     BatchNorm2d-279             [-1, 48, 8, 8]              96\n",
      "            ReLU-280             [-1, 48, 8, 8]               0\n",
      "          Conv2d-281             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-282            [-1, 234, 8, 8]               0\n",
      "     BatchNorm2d-283            [-1, 234, 8, 8]             468\n",
      "            ReLU-284            [-1, 234, 8, 8]               0\n",
      "          Conv2d-285             [-1, 48, 8, 8]          11,232\n",
      "     BatchNorm2d-286             [-1, 48, 8, 8]              96\n",
      "            ReLU-287             [-1, 48, 8, 8]               0\n",
      "          Conv2d-288             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-289            [-1, 246, 8, 8]               0\n",
      "     BatchNorm2d-290            [-1, 246, 8, 8]             492\n",
      "            ReLU-291            [-1, 246, 8, 8]               0\n",
      "          Conv2d-292             [-1, 48, 8, 8]          11,808\n",
      "     BatchNorm2d-293             [-1, 48, 8, 8]              96\n",
      "            ReLU-294             [-1, 48, 8, 8]               0\n",
      "          Conv2d-295             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-296            [-1, 258, 8, 8]               0\n",
      "     BatchNorm2d-297            [-1, 258, 8, 8]             516\n",
      "            ReLU-298            [-1, 258, 8, 8]               0\n",
      "          Conv2d-299             [-1, 48, 8, 8]          12,384\n",
      "     BatchNorm2d-300             [-1, 48, 8, 8]              96\n",
      "            ReLU-301             [-1, 48, 8, 8]               0\n",
      "          Conv2d-302             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-303            [-1, 270, 8, 8]               0\n",
      "     BatchNorm2d-304            [-1, 270, 8, 8]             540\n",
      "            ReLU-305            [-1, 270, 8, 8]               0\n",
      "          Conv2d-306             [-1, 48, 8, 8]          12,960\n",
      "     BatchNorm2d-307             [-1, 48, 8, 8]              96\n",
      "            ReLU-308             [-1, 48, 8, 8]               0\n",
      "          Conv2d-309             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-310            [-1, 282, 8, 8]               0\n",
      "     BatchNorm2d-311            [-1, 282, 8, 8]             564\n",
      "            ReLU-312            [-1, 282, 8, 8]               0\n",
      "          Conv2d-313             [-1, 48, 8, 8]          13,536\n",
      "     BatchNorm2d-314             [-1, 48, 8, 8]              96\n",
      "            ReLU-315             [-1, 48, 8, 8]               0\n",
      "          Conv2d-316             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-317            [-1, 294, 8, 8]               0\n",
      "     BatchNorm2d-318            [-1, 294, 8, 8]             588\n",
      "            ReLU-319            [-1, 294, 8, 8]               0\n",
      "          Conv2d-320             [-1, 48, 8, 8]          14,112\n",
      "     BatchNorm2d-321             [-1, 48, 8, 8]              96\n",
      "            ReLU-322             [-1, 48, 8, 8]               0\n",
      "          Conv2d-323             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-324            [-1, 306, 8, 8]               0\n",
      "     BatchNorm2d-325            [-1, 306, 8, 8]             612\n",
      "            ReLU-326            [-1, 306, 8, 8]               0\n",
      "          Conv2d-327             [-1, 48, 8, 8]          14,688\n",
      "     BatchNorm2d-328             [-1, 48, 8, 8]              96\n",
      "            ReLU-329             [-1, 48, 8, 8]               0\n",
      "          Conv2d-330             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-331            [-1, 318, 8, 8]               0\n",
      "     BatchNorm2d-332            [-1, 318, 8, 8]             636\n",
      "            ReLU-333            [-1, 318, 8, 8]               0\n",
      "          Conv2d-334             [-1, 48, 8, 8]          15,264\n",
      "     BatchNorm2d-335             [-1, 48, 8, 8]              96\n",
      "            ReLU-336             [-1, 48, 8, 8]               0\n",
      "          Conv2d-337             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-338            [-1, 330, 8, 8]               0\n",
      "     BatchNorm2d-339            [-1, 330, 8, 8]             660\n",
      "            ReLU-340            [-1, 330, 8, 8]               0\n",
      "          Conv2d-341             [-1, 48, 8, 8]          15,840\n",
      "     BatchNorm2d-342             [-1, 48, 8, 8]              96\n",
      "            ReLU-343             [-1, 48, 8, 8]               0\n",
      "          Conv2d-344             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-345            [-1, 342, 8, 8]               0\n",
      "     BatchNorm2d-346            [-1, 342, 8, 8]             684\n",
      "            ReLU-347            [-1, 342, 8, 8]               0\n",
      "       AvgPool2d-348            [-1, 342, 1, 1]               0\n",
      "          Linear-349                  [-1, 128]          43,904\n",
      "          Linear-350                   [-1, 10]           1,290\n",
      "        Head_Net-351      [[-1, 128], [-1, 10]]               0\n",
      "================================================================\n",
      "Total params: 810,926\n",
      "Trainable params: 810,926\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 100.98\n",
      "Params size (MB): 3.09\n",
      "Estimated Total Size (MB): 104.09\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model.cuda(), input_size=(3, 32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 24, 32, 32]             648\n",
      "       BatchNorm2d-2           [-1, 24, 32, 32]              48\n",
      "              ReLU-3           [-1, 24, 32, 32]               0\n",
      "            Conv2d-4           [-1, 48, 32, 32]           1,152\n",
      "       BatchNorm2d-5           [-1, 48, 32, 32]              96\n",
      "              ReLU-6           [-1, 48, 32, 32]               0\n",
      "            Conv2d-7           [-1, 12, 32, 32]           5,184\n",
      "        Bottleneck-8           [-1, 36, 32, 32]               0\n",
      "       BatchNorm2d-9           [-1, 36, 32, 32]              72\n",
      "             ReLU-10           [-1, 36, 32, 32]               0\n",
      "           Conv2d-11           [-1, 48, 32, 32]           1,728\n",
      "      BatchNorm2d-12           [-1, 48, 32, 32]              96\n",
      "             ReLU-13           [-1, 48, 32, 32]               0\n",
      "           Conv2d-14           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-15           [-1, 48, 32, 32]               0\n",
      "      BatchNorm2d-16           [-1, 48, 32, 32]              96\n",
      "             ReLU-17           [-1, 48, 32, 32]               0\n",
      "           Conv2d-18           [-1, 48, 32, 32]           2,304\n",
      "      BatchNorm2d-19           [-1, 48, 32, 32]              96\n",
      "             ReLU-20           [-1, 48, 32, 32]               0\n",
      "           Conv2d-21           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-22           [-1, 60, 32, 32]               0\n",
      "      BatchNorm2d-23           [-1, 60, 32, 32]             120\n",
      "             ReLU-24           [-1, 60, 32, 32]               0\n",
      "           Conv2d-25           [-1, 48, 32, 32]           2,880\n",
      "      BatchNorm2d-26           [-1, 48, 32, 32]              96\n",
      "             ReLU-27           [-1, 48, 32, 32]               0\n",
      "           Conv2d-28           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-29           [-1, 72, 32, 32]               0\n",
      "      BatchNorm2d-30           [-1, 72, 32, 32]             144\n",
      "             ReLU-31           [-1, 72, 32, 32]               0\n",
      "           Conv2d-32           [-1, 48, 32, 32]           3,456\n",
      "      BatchNorm2d-33           [-1, 48, 32, 32]              96\n",
      "             ReLU-34           [-1, 48, 32, 32]               0\n",
      "           Conv2d-35           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-36           [-1, 84, 32, 32]               0\n",
      "      BatchNorm2d-37           [-1, 84, 32, 32]             168\n",
      "             ReLU-38           [-1, 84, 32, 32]               0\n",
      "           Conv2d-39           [-1, 48, 32, 32]           4,032\n",
      "      BatchNorm2d-40           [-1, 48, 32, 32]              96\n",
      "             ReLU-41           [-1, 48, 32, 32]               0\n",
      "           Conv2d-42           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-43           [-1, 96, 32, 32]               0\n",
      "      BatchNorm2d-44           [-1, 96, 32, 32]             192\n",
      "             ReLU-45           [-1, 96, 32, 32]               0\n",
      "           Conv2d-46           [-1, 48, 32, 32]           4,608\n",
      "      BatchNorm2d-47           [-1, 48, 32, 32]              96\n",
      "             ReLU-48           [-1, 48, 32, 32]               0\n",
      "           Conv2d-49           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-50          [-1, 108, 32, 32]               0\n",
      "      BatchNorm2d-51          [-1, 108, 32, 32]             216\n",
      "             ReLU-52          [-1, 108, 32, 32]               0\n",
      "           Conv2d-53           [-1, 48, 32, 32]           5,184\n",
      "      BatchNorm2d-54           [-1, 48, 32, 32]              96\n",
      "             ReLU-55           [-1, 48, 32, 32]               0\n",
      "           Conv2d-56           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-57          [-1, 120, 32, 32]               0\n",
      "      BatchNorm2d-58          [-1, 120, 32, 32]             240\n",
      "             ReLU-59          [-1, 120, 32, 32]               0\n",
      "           Conv2d-60           [-1, 48, 32, 32]           5,760\n",
      "      BatchNorm2d-61           [-1, 48, 32, 32]              96\n",
      "             ReLU-62           [-1, 48, 32, 32]               0\n",
      "           Conv2d-63           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-64          [-1, 132, 32, 32]               0\n",
      "      BatchNorm2d-65          [-1, 132, 32, 32]             264\n",
      "             ReLU-66          [-1, 132, 32, 32]               0\n",
      "           Conv2d-67           [-1, 48, 32, 32]           6,336\n",
      "      BatchNorm2d-68           [-1, 48, 32, 32]              96\n",
      "             ReLU-69           [-1, 48, 32, 32]               0\n",
      "           Conv2d-70           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-71          [-1, 144, 32, 32]               0\n",
      "      BatchNorm2d-72          [-1, 144, 32, 32]             288\n",
      "             ReLU-73          [-1, 144, 32, 32]               0\n",
      "           Conv2d-74           [-1, 48, 32, 32]           6,912\n",
      "      BatchNorm2d-75           [-1, 48, 32, 32]              96\n",
      "             ReLU-76           [-1, 48, 32, 32]               0\n",
      "           Conv2d-77           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-78          [-1, 156, 32, 32]               0\n",
      "      BatchNorm2d-79          [-1, 156, 32, 32]             312\n",
      "             ReLU-80          [-1, 156, 32, 32]               0\n",
      "           Conv2d-81           [-1, 48, 32, 32]           7,488\n",
      "      BatchNorm2d-82           [-1, 48, 32, 32]              96\n",
      "             ReLU-83           [-1, 48, 32, 32]               0\n",
      "           Conv2d-84           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-85          [-1, 168, 32, 32]               0\n",
      "      BatchNorm2d-86          [-1, 168, 32, 32]             336\n",
      "             ReLU-87          [-1, 168, 32, 32]               0\n",
      "           Conv2d-88           [-1, 48, 32, 32]           8,064\n",
      "      BatchNorm2d-89           [-1, 48, 32, 32]              96\n",
      "             ReLU-90           [-1, 48, 32, 32]               0\n",
      "           Conv2d-91           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-92          [-1, 180, 32, 32]               0\n",
      "      BatchNorm2d-93          [-1, 180, 32, 32]             360\n",
      "             ReLU-94          [-1, 180, 32, 32]               0\n",
      "           Conv2d-95           [-1, 48, 32, 32]           8,640\n",
      "      BatchNorm2d-96           [-1, 48, 32, 32]              96\n",
      "             ReLU-97           [-1, 48, 32, 32]               0\n",
      "           Conv2d-98           [-1, 12, 32, 32]           5,184\n",
      "       Bottleneck-99          [-1, 192, 32, 32]               0\n",
      "     BatchNorm2d-100          [-1, 192, 32, 32]             384\n",
      "            ReLU-101          [-1, 192, 32, 32]               0\n",
      "          Conv2d-102           [-1, 48, 32, 32]           9,216\n",
      "     BatchNorm2d-103           [-1, 48, 32, 32]              96\n",
      "            ReLU-104           [-1, 48, 32, 32]               0\n",
      "          Conv2d-105           [-1, 12, 32, 32]           5,184\n",
      "      Bottleneck-106          [-1, 204, 32, 32]               0\n",
      "     BatchNorm2d-107          [-1, 204, 32, 32]             408\n",
      "            ReLU-108          [-1, 204, 32, 32]               0\n",
      "          Conv2d-109           [-1, 48, 32, 32]           9,792\n",
      "     BatchNorm2d-110           [-1, 48, 32, 32]              96\n",
      "            ReLU-111           [-1, 48, 32, 32]               0\n",
      "          Conv2d-112           [-1, 12, 32, 32]           5,184\n",
      "      Bottleneck-113          [-1, 216, 32, 32]               0\n",
      "     BatchNorm2d-114          [-1, 216, 32, 32]             432\n",
      "            ReLU-115          [-1, 216, 32, 32]               0\n",
      "          Conv2d-116          [-1, 108, 32, 32]          23,328\n",
      "      Transition-117          [-1, 108, 16, 16]               0\n",
      "     BatchNorm2d-118          [-1, 108, 16, 16]             216\n",
      "            ReLU-119          [-1, 108, 16, 16]               0\n",
      "          Conv2d-120           [-1, 48, 16, 16]           5,184\n",
      "     BatchNorm2d-121           [-1, 48, 16, 16]              96\n",
      "            ReLU-122           [-1, 48, 16, 16]               0\n",
      "          Conv2d-123           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-124          [-1, 120, 16, 16]               0\n",
      "     BatchNorm2d-125          [-1, 120, 16, 16]             240\n",
      "            ReLU-126          [-1, 120, 16, 16]               0\n",
      "          Conv2d-127           [-1, 48, 16, 16]           5,760\n",
      "     BatchNorm2d-128           [-1, 48, 16, 16]              96\n",
      "            ReLU-129           [-1, 48, 16, 16]               0\n",
      "          Conv2d-130           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-131          [-1, 132, 16, 16]               0\n",
      "     BatchNorm2d-132          [-1, 132, 16, 16]             264\n",
      "            ReLU-133          [-1, 132, 16, 16]               0\n",
      "          Conv2d-134           [-1, 48, 16, 16]           6,336\n",
      "     BatchNorm2d-135           [-1, 48, 16, 16]              96\n",
      "            ReLU-136           [-1, 48, 16, 16]               0\n",
      "          Conv2d-137           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-138          [-1, 144, 16, 16]               0\n",
      "     BatchNorm2d-139          [-1, 144, 16, 16]             288\n",
      "            ReLU-140          [-1, 144, 16, 16]               0\n",
      "          Conv2d-141           [-1, 48, 16, 16]           6,912\n",
      "     BatchNorm2d-142           [-1, 48, 16, 16]              96\n",
      "            ReLU-143           [-1, 48, 16, 16]               0\n",
      "          Conv2d-144           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-145          [-1, 156, 16, 16]               0\n",
      "     BatchNorm2d-146          [-1, 156, 16, 16]             312\n",
      "            ReLU-147          [-1, 156, 16, 16]               0\n",
      "          Conv2d-148           [-1, 48, 16, 16]           7,488\n",
      "     BatchNorm2d-149           [-1, 48, 16, 16]              96\n",
      "            ReLU-150           [-1, 48, 16, 16]               0\n",
      "          Conv2d-151           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-152          [-1, 168, 16, 16]               0\n",
      "     BatchNorm2d-153          [-1, 168, 16, 16]             336\n",
      "            ReLU-154          [-1, 168, 16, 16]               0\n",
      "          Conv2d-155           [-1, 48, 16, 16]           8,064\n",
      "     BatchNorm2d-156           [-1, 48, 16, 16]              96\n",
      "            ReLU-157           [-1, 48, 16, 16]               0\n",
      "          Conv2d-158           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-159          [-1, 180, 16, 16]               0\n",
      "     BatchNorm2d-160          [-1, 180, 16, 16]             360\n",
      "            ReLU-161          [-1, 180, 16, 16]               0\n",
      "          Conv2d-162           [-1, 48, 16, 16]           8,640\n",
      "     BatchNorm2d-163           [-1, 48, 16, 16]              96\n",
      "            ReLU-164           [-1, 48, 16, 16]               0\n",
      "          Conv2d-165           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-166          [-1, 192, 16, 16]               0\n",
      "     BatchNorm2d-167          [-1, 192, 16, 16]             384\n",
      "            ReLU-168          [-1, 192, 16, 16]               0\n",
      "          Conv2d-169           [-1, 48, 16, 16]           9,216\n",
      "     BatchNorm2d-170           [-1, 48, 16, 16]              96\n",
      "            ReLU-171           [-1, 48, 16, 16]               0\n",
      "          Conv2d-172           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-173          [-1, 204, 16, 16]               0\n",
      "     BatchNorm2d-174          [-1, 204, 16, 16]             408\n",
      "            ReLU-175          [-1, 204, 16, 16]               0\n",
      "          Conv2d-176           [-1, 48, 16, 16]           9,792\n",
      "     BatchNorm2d-177           [-1, 48, 16, 16]              96\n",
      "            ReLU-178           [-1, 48, 16, 16]               0\n",
      "          Conv2d-179           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-180          [-1, 216, 16, 16]               0\n",
      "     BatchNorm2d-181          [-1, 216, 16, 16]             432\n",
      "            ReLU-182          [-1, 216, 16, 16]               0\n",
      "          Conv2d-183           [-1, 48, 16, 16]          10,368\n",
      "     BatchNorm2d-184           [-1, 48, 16, 16]              96\n",
      "            ReLU-185           [-1, 48, 16, 16]               0\n",
      "          Conv2d-186           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-187          [-1, 228, 16, 16]               0\n",
      "     BatchNorm2d-188          [-1, 228, 16, 16]             456\n",
      "            ReLU-189          [-1, 228, 16, 16]               0\n",
      "          Conv2d-190           [-1, 48, 16, 16]          10,944\n",
      "     BatchNorm2d-191           [-1, 48, 16, 16]              96\n",
      "            ReLU-192           [-1, 48, 16, 16]               0\n",
      "          Conv2d-193           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-194          [-1, 240, 16, 16]               0\n",
      "     BatchNorm2d-195          [-1, 240, 16, 16]             480\n",
      "            ReLU-196          [-1, 240, 16, 16]               0\n",
      "          Conv2d-197           [-1, 48, 16, 16]          11,520\n",
      "     BatchNorm2d-198           [-1, 48, 16, 16]              96\n",
      "            ReLU-199           [-1, 48, 16, 16]               0\n",
      "          Conv2d-200           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-201          [-1, 252, 16, 16]               0\n",
      "     BatchNorm2d-202          [-1, 252, 16, 16]             504\n",
      "            ReLU-203          [-1, 252, 16, 16]               0\n",
      "          Conv2d-204           [-1, 48, 16, 16]          12,096\n",
      "     BatchNorm2d-205           [-1, 48, 16, 16]              96\n",
      "            ReLU-206           [-1, 48, 16, 16]               0\n",
      "          Conv2d-207           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-208          [-1, 264, 16, 16]               0\n",
      "     BatchNorm2d-209          [-1, 264, 16, 16]             528\n",
      "            ReLU-210          [-1, 264, 16, 16]               0\n",
      "          Conv2d-211           [-1, 48, 16, 16]          12,672\n",
      "     BatchNorm2d-212           [-1, 48, 16, 16]              96\n",
      "            ReLU-213           [-1, 48, 16, 16]               0\n",
      "          Conv2d-214           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-215          [-1, 276, 16, 16]               0\n",
      "     BatchNorm2d-216          [-1, 276, 16, 16]             552\n",
      "            ReLU-217          [-1, 276, 16, 16]               0\n",
      "          Conv2d-218           [-1, 48, 16, 16]          13,248\n",
      "     BatchNorm2d-219           [-1, 48, 16, 16]              96\n",
      "            ReLU-220           [-1, 48, 16, 16]               0\n",
      "          Conv2d-221           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-222          [-1, 288, 16, 16]               0\n",
      "     BatchNorm2d-223          [-1, 288, 16, 16]             576\n",
      "            ReLU-224          [-1, 288, 16, 16]               0\n",
      "          Conv2d-225           [-1, 48, 16, 16]          13,824\n",
      "     BatchNorm2d-226           [-1, 48, 16, 16]              96\n",
      "            ReLU-227           [-1, 48, 16, 16]               0\n",
      "          Conv2d-228           [-1, 12, 16, 16]           5,184\n",
      "      Bottleneck-229          [-1, 300, 16, 16]               0\n",
      "     BatchNorm2d-230          [-1, 300, 16, 16]             600\n",
      "            ReLU-231          [-1, 300, 16, 16]               0\n",
      "          Conv2d-232          [-1, 150, 16, 16]          45,000\n",
      "      Transition-233            [-1, 150, 8, 8]               0\n",
      "     BatchNorm2d-234            [-1, 150, 8, 8]             300\n",
      "            ReLU-235            [-1, 150, 8, 8]               0\n",
      "          Conv2d-236             [-1, 48, 8, 8]           7,200\n",
      "     BatchNorm2d-237             [-1, 48, 8, 8]              96\n",
      "            ReLU-238             [-1, 48, 8, 8]               0\n",
      "          Conv2d-239             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-240            [-1, 162, 8, 8]               0\n",
      "     BatchNorm2d-241            [-1, 162, 8, 8]             324\n",
      "            ReLU-242            [-1, 162, 8, 8]               0\n",
      "          Conv2d-243             [-1, 48, 8, 8]           7,776\n",
      "     BatchNorm2d-244             [-1, 48, 8, 8]              96\n",
      "            ReLU-245             [-1, 48, 8, 8]               0\n",
      "          Conv2d-246             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-247            [-1, 174, 8, 8]               0\n",
      "     BatchNorm2d-248            [-1, 174, 8, 8]             348\n",
      "            ReLU-249            [-1, 174, 8, 8]               0\n",
      "          Conv2d-250             [-1, 48, 8, 8]           8,352\n",
      "     BatchNorm2d-251             [-1, 48, 8, 8]              96\n",
      "            ReLU-252             [-1, 48, 8, 8]               0\n",
      "          Conv2d-253             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-254            [-1, 186, 8, 8]               0\n",
      "     BatchNorm2d-255            [-1, 186, 8, 8]             372\n",
      "            ReLU-256            [-1, 186, 8, 8]               0\n",
      "          Conv2d-257             [-1, 48, 8, 8]           8,928\n",
      "     BatchNorm2d-258             [-1, 48, 8, 8]              96\n",
      "            ReLU-259             [-1, 48, 8, 8]               0\n",
      "          Conv2d-260             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-261            [-1, 198, 8, 8]               0\n",
      "     BatchNorm2d-262            [-1, 198, 8, 8]             396\n",
      "            ReLU-263            [-1, 198, 8, 8]               0\n",
      "          Conv2d-264             [-1, 48, 8, 8]           9,504\n",
      "     BatchNorm2d-265             [-1, 48, 8, 8]              96\n",
      "            ReLU-266             [-1, 48, 8, 8]               0\n",
      "          Conv2d-267             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-268            [-1, 210, 8, 8]               0\n",
      "     BatchNorm2d-269            [-1, 210, 8, 8]             420\n",
      "            ReLU-270            [-1, 210, 8, 8]               0\n",
      "          Conv2d-271             [-1, 48, 8, 8]          10,080\n",
      "     BatchNorm2d-272             [-1, 48, 8, 8]              96\n",
      "            ReLU-273             [-1, 48, 8, 8]               0\n",
      "          Conv2d-274             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-275            [-1, 222, 8, 8]               0\n",
      "     BatchNorm2d-276            [-1, 222, 8, 8]             444\n",
      "            ReLU-277            [-1, 222, 8, 8]               0\n",
      "          Conv2d-278             [-1, 48, 8, 8]          10,656\n",
      "     BatchNorm2d-279             [-1, 48, 8, 8]              96\n",
      "            ReLU-280             [-1, 48, 8, 8]               0\n",
      "          Conv2d-281             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-282            [-1, 234, 8, 8]               0\n",
      "     BatchNorm2d-283            [-1, 234, 8, 8]             468\n",
      "            ReLU-284            [-1, 234, 8, 8]               0\n",
      "          Conv2d-285             [-1, 48, 8, 8]          11,232\n",
      "     BatchNorm2d-286             [-1, 48, 8, 8]              96\n",
      "            ReLU-287             [-1, 48, 8, 8]               0\n",
      "          Conv2d-288             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-289            [-1, 246, 8, 8]               0\n",
      "     BatchNorm2d-290            [-1, 246, 8, 8]             492\n",
      "            ReLU-291            [-1, 246, 8, 8]               0\n",
      "          Conv2d-292             [-1, 48, 8, 8]          11,808\n",
      "     BatchNorm2d-293             [-1, 48, 8, 8]              96\n",
      "            ReLU-294             [-1, 48, 8, 8]               0\n",
      "          Conv2d-295             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-296            [-1, 258, 8, 8]               0\n",
      "     BatchNorm2d-297            [-1, 258, 8, 8]             516\n",
      "            ReLU-298            [-1, 258, 8, 8]               0\n",
      "          Conv2d-299             [-1, 48, 8, 8]          12,384\n",
      "     BatchNorm2d-300             [-1, 48, 8, 8]              96\n",
      "            ReLU-301             [-1, 48, 8, 8]               0\n",
      "          Conv2d-302             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-303            [-1, 270, 8, 8]               0\n",
      "     BatchNorm2d-304            [-1, 270, 8, 8]             540\n",
      "            ReLU-305            [-1, 270, 8, 8]               0\n",
      "          Conv2d-306             [-1, 48, 8, 8]          12,960\n",
      "     BatchNorm2d-307             [-1, 48, 8, 8]              96\n",
      "            ReLU-308             [-1, 48, 8, 8]               0\n",
      "          Conv2d-309             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-310            [-1, 282, 8, 8]               0\n",
      "     BatchNorm2d-311            [-1, 282, 8, 8]             564\n",
      "            ReLU-312            [-1, 282, 8, 8]               0\n",
      "          Conv2d-313             [-1, 48, 8, 8]          13,536\n",
      "     BatchNorm2d-314             [-1, 48, 8, 8]              96\n",
      "            ReLU-315             [-1, 48, 8, 8]               0\n",
      "          Conv2d-316             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-317            [-1, 294, 8, 8]               0\n",
      "     BatchNorm2d-318            [-1, 294, 8, 8]             588\n",
      "            ReLU-319            [-1, 294, 8, 8]               0\n",
      "          Conv2d-320             [-1, 48, 8, 8]          14,112\n",
      "     BatchNorm2d-321             [-1, 48, 8, 8]              96\n",
      "            ReLU-322             [-1, 48, 8, 8]               0\n",
      "          Conv2d-323             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-324            [-1, 306, 8, 8]               0\n",
      "     BatchNorm2d-325            [-1, 306, 8, 8]             612\n",
      "            ReLU-326            [-1, 306, 8, 8]               0\n",
      "          Conv2d-327             [-1, 48, 8, 8]          14,688\n",
      "     BatchNorm2d-328             [-1, 48, 8, 8]              96\n",
      "            ReLU-329             [-1, 48, 8, 8]               0\n",
      "          Conv2d-330             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-331            [-1, 318, 8, 8]               0\n",
      "     BatchNorm2d-332            [-1, 318, 8, 8]             636\n",
      "            ReLU-333            [-1, 318, 8, 8]               0\n",
      "          Conv2d-334             [-1, 48, 8, 8]          15,264\n",
      "     BatchNorm2d-335             [-1, 48, 8, 8]              96\n",
      "            ReLU-336             [-1, 48, 8, 8]               0\n",
      "          Conv2d-337             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-338            [-1, 330, 8, 8]               0\n",
      "     BatchNorm2d-339            [-1, 330, 8, 8]             660\n",
      "            ReLU-340            [-1, 330, 8, 8]               0\n",
      "          Conv2d-341             [-1, 48, 8, 8]          15,840\n",
      "     BatchNorm2d-342             [-1, 48, 8, 8]              96\n",
      "            ReLU-343             [-1, 48, 8, 8]               0\n",
      "          Conv2d-344             [-1, 12, 8, 8]           5,184\n",
      "      Bottleneck-345            [-1, 342, 8, 8]               0\n",
      "     BatchNorm2d-346            [-1, 342, 8, 8]             684\n",
      "            ReLU-347            [-1, 342, 8, 8]               0\n",
      "       AvgPool2d-348            [-1, 342, 1, 1]               0\n",
      "          Linear-349                  [-1, 128]          43,904\n",
      "          Linear-350                   [-1, 10]           1,290\n",
      "        Head_Net-351      [[-1, 128], [-1, 10]]               0\n",
      "================================================================\n",
      "Total params: 810,926\n",
      "Trainable params: 810,926\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 100.98\n",
      "Params size (MB): 3.09\n",
      "Estimated Total Size (MB): 104.09\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model_final,input_size=(3, 32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
